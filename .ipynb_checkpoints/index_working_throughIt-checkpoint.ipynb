{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mccalister = ['Adam', 'Amanda','Chum', 'Dann',\n",
    " 'Jacob', 'Jason', 'Johnhoy', 'Karim',\n",
    "'Leana','Luluva', 'Matt', 'Maximilian','Syd' ]\n",
    "\n",
    "# This is always a good idea\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src.student_caller import one_random_student\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8EQBfbRRV7w"
   },
   "source": [
    "## Modeling\n",
    "\n",
    "Let's review some modeling concepts we've used to date with [this quick exercise](https://forms.gle/yrPxUp2Xj4R9FeyEA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hko6Q989RV7w"
   },
   "source": [
    "We do this to remind ourselves that the basic components of good modeling practice, and even the methods themselves, are _the same_ with Neural Nets as that are with _sklearn_ or _statsmodels_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xv_Ypg88RV7x"
   },
   "source": [
    "## Objectives:\n",
    "- hands on practice defining two separate NN\n",
    "- Compare pros and cons of Keras vs TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwkD4T2eRV7y"
   },
   "source": [
    "### Think, Pair, Share Challenge:\n",
    "\n",
    "<img src=\"https://images.pexels.com/photos/1350560/pexels-photo-1350560.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\" alt=\"diabetes\" style =\"text-align:center;width:250px;float:none\" ></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue where we left off with our numbers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a binary classification, and predict whether the number will be even or odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary = y % 2\n",
    "y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1:\n",
    "Questions to answer:\n",
    "- How many input variables are there in this dataset? \n",
    "- What does the range of values (0-16) represent in our feature set?\n",
    "- What does a 1 mean in our target class?\n",
    "- If we use a neural net to predict this, what loss function do we use?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Part 2:\n",
    "What if you wanted to create a NN with hidden layers to predict who will get diabetes?\n",
    "- 12 nodes in the first hidden layer\n",
    "- 8 nodes in the second hidden layer\n",
    "- relu on the first two activations\n",
    "- sigmoid on the last one\n",
    "- How many nodes in the input layer?\n",
    "- How many nodes in the output layer?\n",
    "- Will the output layer produce an integer or a float?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Part 3:\n",
    "Knowing that you want:\n",
    "- batch size of 10\n",
    "- 50 epochs\n",
    "- to use `rmsprop` as your optimizer\n",
    "- and all the numbers you defined above...\n",
    "\n",
    "**Fill out the code below with the correct specifications, but don't run it yet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-135e9df370ed>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-135e9df370ed>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    model.add(Dense(   , activation= , input_dim= ))\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(   , activation= , input_dim= ))\n",
    "model.add(Dense( ,  activation= ))\n",
    "model.add(Dense(  , activation =  ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, epochs=50, batch_size= 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=64,))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(1 , activation = 'sigmoid' ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, y_binary, epochs=50, batch_size= 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to know:\n",
    "- the data and labels in `fit()` need to be numpy arrays, not pandas dfs. Else it won't work.\n",
    "- Scaling your data will have a large impact on your model. Let's use standard scaler .\n",
    "\n",
    "![gif](https://media0.giphy.com/media/3og0IMJcSI8p6hYQXS/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting data ready for modeling\n",
    "**Tasks**:\n",
    "\n",
    "- use train_test_split to create X_train, y_train, X_test, and y_test\n",
    "- Split training data into train and validation sets.\n",
    "- Scale the pixel intensity to a value between 0 and 1.\n",
    "- Scale the pixel intensity to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data for neural networks is very important, whether it be for image processing or prediction problems like we've seen in past projects and lessons.  \n",
    "\n",
    "Scaling our input variables will help speed up our neural network [see 4.3](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "Since our minimum intensity is 0, we can normalize the inputs by dividing each value by the max value (16). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__SOLUTION__\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, random_state=42, test_size=.2)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state=42, test_size=.2)\n",
    "X_t, X_val, X_test = X_t/16, X_val/16, X_test/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=64,))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(1 , activation = 'sigmoid' ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_t, y_t, epochs=50, batch_size= 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=64,))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(1 , activation = 'sigmoid' ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Assigne the variable history to store the results\n",
    "results = model.fit(X_t, y_t, epochs=50, batch_size= 10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = results.history\n",
    "training_loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "training_accuracy = history['acc']\n",
    "val_accuracy = history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "sns.lineplot(list(range(len(training_loss))), training_loss, c='r', label='training', ax=ax1)\n",
    "sns.lineplot(list(range(len(val_loss))), val_loss, c='b', label='validation', ax=ax1)\n",
    "sns.lineplot(list(range(len(training_loss))), training_accuracy, c='r', label='training',ax=ax2)\n",
    "sns.lineplot(list(range(len(val_loss))), val_accuracy, c='b', label='validation',ax=ax2)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "y_hat_test = model.predict(X_test)\n",
    "y_hat_test = (y_hat_test > .5).astype(int)\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_hat_test)}')\n",
    "print(f'Recall: {recall_score(y_test, y_hat_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's return to the original problem: predicting 0 through 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.2)\n",
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train, random_state=42, test_size=.2)\n",
    "X_t, X_val, X_test = X_t/16, X_val/16, X_test/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "y_t = ohe.fit_transform(y_t.reshape(-1,1))\n",
    "y_val = ohe.transform(y_val.reshape(-1,1))\n",
    "y_test = ohe.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=64,))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(10 , activation = 'softmax' ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='categorical_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "results = model.fit(X_t, y_t, epochs=50, batch_size= 10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = results.history\n",
    "training_loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "training_accuracy = history['acc']\n",
    "val_accuracy = history['val_acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "\n",
    "sns.lineplot(list(range(len(training_loss))), training_loss, c='r', label='training', ax=ax1)\n",
    "sns.lineplot(list(range(len(val_loss))), val_loss, c='b', label='validation', ax=ax1)\n",
    "sns.lineplot(list(range(len(training_loss))), training_accuracy, c='r', label='training',ax=ax2)\n",
    "sns.lineplot(list(range(len(val_loss))), val_accuracy, c='b', label='validation',ax=ax2)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "y_test = ohe.inverse_transform(y_test)\n",
    "confusion_matrix(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "\n",
    "train_data_dir = 'dogvcat/dataset/training_set'\n",
    "test_data_dir = 'dogvcat/dataset/test_set'\n",
    "\n",
    "# Get all the data in the directory data/validation (132 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_data_dir, \n",
    "        target_size=(64, 64), batch_size=1000)\n",
    "\n",
    "# Get all the data in the directory data/train (790 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(64, 64), batch_size=5000)\n",
    "\n",
    "# Create the datasets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "array_to_img(train_images[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_img(train_images[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_images = train_images.reshape(train_images.shape[0], -1)\n",
    "te_images = test_images.reshape(test_images.shape[0], -1)\n",
    "tr_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=12288,))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(3 ,  activation='relu' ))\n",
    "model.add(Dense(2 , activation = 'sigmoid' ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "results = model.fit(tr_images, train_labels, epochs=100, batch_size= 10,\n",
    "                    validation_data=(te_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, activation='relu', input_dim=12288,))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(2 , activation = 'sigmoid' ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "results = model.fit(tr_images, train_labels, epochs=50, batch_size= 10,\n",
    "                    validation_data=(te_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwkD4T2eRV7y"
   },
   "source": [
    "We want to see if we can predict diabetes from input variables\n",
    "\n",
    "Let's start with a dataset we are familiar with. Imaging that you have applied all of the algorithms in your toolkit, and can't achieve an acceptable metric.\n",
    "\n",
    "Here is the code to grab the dataset. Please use `pandas` to answer some of the questions:\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://query.data.world/s/hbvdf6y2dqlpzjsc5ho5yboxy3tz7l')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://query.data.world/s/hbvdf6y2dqlpzjsc5ho5yboxy3tz7l')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwkD4T2eRV7y"
   },
   "source": [
    "#### Part 1:\n",
    "Questions to answer:\n",
    "- How many input variables are there in this dataset? \n",
    "- Which variable is the target? \n",
    "- Do we need to drop any columns from the dataset?\n",
    "- If we use a neural net to predict this, what loss function do we use?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwkD4T2eRV7y"
   },
   "source": [
    "***\n",
    "#### Part 2:\n",
    "What if you wanted to create a NN with hidden layers to predict who will get diabetes?\n",
    "- 12 nodes in the first hidden layer\n",
    "- 8 nodes in the second hidden layer\n",
    "- relu on the first two activations\n",
    "- sigmoid on the last one\n",
    "- How many nodes in the input layer?\n",
    "- How many nodes in the output layer?\n",
    "- Will the output layer produce an integer or a float?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwkD4T2eRV7y"
   },
   "source": [
    "***\n",
    "\n",
    "#### Part 3:\n",
    "Knowing that you want:\n",
    "- batch size of 10\n",
    "- 50 epochs\n",
    "- to use `rmsprop` as your optimizer\n",
    "- and all the numbers you defined above...\n",
    "\n",
    "**Fill out the code below with the correct specifications, but don't run it yet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(   , activation= , input_dim= ))\n",
    "model.add(Dense( ,  activation= ))\n",
    "model.add(Dense(  , activation =  ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, epochs=50, batch_size= 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(   , activation= , input_dim= ))\n",
    "model.add(Dense( ,  activation= ))\n",
    "model.add(Dense(  , activation =  ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, epochs=50, batch_size= 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://query.data.world/s/hbvdf6y2dqlpzjsc5ho5yboxy3tz7l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_ZiMAKhRV7z"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12   , activation='relu' , input_dim=(9,)  ))\n",
    "model.add(Dense(8 ,  activation='relu' ))\n",
    "model.add(Dense(1  , activation ='sigmoid'  ))\n",
    "\n",
    "model.compile(optimizer=adam ,\n",
    "              loss='binary_crossentropy'  ,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(data, labels, epochs=50, batch_size= 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViK7OzPWRV72"
   },
   "source": [
    "### Things to know:\n",
    "- the data and labels in `fit()` need to be numpy arrays, not pandas dfs. Else it won't work.\n",
    "- Scaling your data will have a large impact on your model. Let's use standard scaler .\n",
    "\n",
    "![gif](https://media0.giphy.com/media/3og0IMJcSI8p6hYQXS/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gu0pJZCdRV73"
   },
   "source": [
    "#### Getting data ready for modeling\n",
    "**Tasks**:\n",
    "- load in the dataset using pandas\n",
    "\n",
    "- convert dataframe to numpy array using the pandas df method [`to_numpy`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy)\n",
    "- separate array into two arrays, `data` and `labels`\n",
    "- use train_test_split to create X_train, y_train, X_test, and y_test\n",
    "- Use standardscaler - instantiate the standard scaler as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DIobhRjRV74"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ss = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fI99AZ1wRV76"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('https://query.data.world/s/hbvdf6y2dqlpzjsc5ho5yboxy3tz7l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfcYYI5fRV78"
   },
   "outputs": [],
   "source": [
    "df_num= df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECUxhE-XRV7_"
   },
   "outputs": [],
   "source": [
    "df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZQnJZw8RV8C"
   },
   "outputs": [],
   "source": [
    "X_data = df_num[:,0:-1]\n",
    "Y_data= df_num[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egoAoDOHRV8Q"
   },
   "source": [
    "**Tasks**:\n",
    "- import appropriate Keras packages\n",
    "- build your model components and compile it\n",
    "- fit your model\n",
    "- report out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_GguqEBBRV8R"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,Y_data, random_state=42, test_size=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ohe.fit(y_train.reshape(-1,1))\n",
    "encoded_y_train = ohe.transform(y_train.reshape(-1,1))\n",
    "encoded_y_test = ohe.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12 , activation= 'relu'  , input_dim=8))\n",
    "model.add(Dense(8,  activation= 'relu' ))\n",
    "model.add(Dense(10,  activation= 'relu' ))\n",
    "model.add(Dense(2 , activation = 'sigmoid' ))\n",
    "\n",
    "model.compile(optimizer= adam,\n",
    "              loss= 'binary_crossentropy' ,\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, encoded_y_train,\n",
    "                    validation_data=(X_test, encoded_y_test), \n",
    "                    verbose = 1, epochs= 50, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "op_NiSh5RV8X"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, verbose=0, epochs= 50, batch_size= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1xHW4E4RV8Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHzCRZWSRV8b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbeLqBVpRV8d"
   },
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Testing Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Wo5VYrjRV8f"
   },
   "outputs": [],
   "source": [
    "train_loss = history.history['accuracy']\n",
    "test_loss = history.history['val_accuracy']\n",
    "plt.plot(train_loss, label='Training accuracy')\n",
    "plt.plot(test_loss, label='Testing Laccuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fffbGE1fRV8i"
   },
   "source": [
    "### Wait, what tool am I even using, what's Keras?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klqIVvjIRV8j"
   },
   "source": [
    "<img align =left src=\"img/keras.png\"><br>\n",
    "### Keras is an API\n",
    "\n",
    "Coded in Python, that can be layered on top of many different back-end processing systems.\n",
    "\n",
    "![kerasback](img/keras_2.png)\n",
    "\n",
    "While each of these systems has their own coding methods, Keras abstracts from that in streamlined pythonic manner we are used to seeing in other python modeling libraries.\n",
    "\n",
    "Keras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as tf.keras. Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is maintaining the Keras fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdZDyJfARV8l"
   },
   "source": [
    "## Wait, what's TensorFlow?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFZYNoYRRV8l"
   },
   "source": [
    "## Let's start with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mIZD7WFvRV8m"
   },
   "source": [
    "## Tensors are multidimensional matricies\n",
    "\n",
    "![tensor](img/tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUblayDlRV8n"
   },
   "source": [
    "### TensorFlow manages the flow of matrix math\n",
    "\n",
    "That makes neural network processing possible.\n",
    "\n",
    "![cat](img/cat-tensors.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UO5h80aARV8n"
   },
   "source": [
    "## TensorFlow at its start\n",
    "\n",
    "An open-source library\n",
    "\n",
    "![more-arch](img/layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBvSl4WhRV8o"
   },
   "source": [
    "### 2007 Keras was fully integrated into TensorFlow\n",
    "\n",
    "It \"comes with\" Tensorflow and provides all the medium to high end API services to integrate with tensorflow processing.\n",
    "\n",
    "![tensorflow-prog](img/tensorflow_programming_environment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTxfhoqMRV8o"
   },
   "source": [
    "### Keras, an API with an intentional UX\n",
    "\n",
    "- Deliberately design end-to-end user workflows\n",
    "- Reduce cognitive load for your users\n",
    "- Provide helpful feedback to your users\n",
    "\n",
    "[full article here](https://blog.keras.io/user-experience-design-for-apis.html)<br>\n",
    "[full list of why to use Keras](https://keras.io/why-use-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTNbwNeRRV8p"
   },
   "source": [
    "### A few comparisons\n",
    "\n",
    "While you **can leverage both**, here are a few comparisons.\n",
    "\n",
    "| Comparison | Keras | Tensorflow|\n",
    "|------------|-------|-----------|\n",
    "| **Level of API** | high-level API | High and low-level APIs |\n",
    "| **Speed** |  can *seem* slower |  is a bit faster |\n",
    "| **Language architecture** | simple architecture, more readable and concise | straight tensorflow is a bit mroe complex |\n",
    "| **Debugging** | less frequent need to debug | difficult to debug |\n",
    "| **Datasets** | usually used for small datasets | high performance models and large datasets that require fast execution|\n",
    "\n",
    "This is also a _**non-issue**_ - as you can leverage tensorflow commands within keras and vice versa. If Keras ever seems slower, it's because the developer's time is more expensive than the GPUs. Keras is designed with the developer in mind. \n",
    "\n",
    "\n",
    "[reference link](https://www.edureka.co/blog/keras-vs-tensorflow-vs-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62GGLUWbRV8p"
   },
   "source": [
    "## More levers and buttons\n",
    "\n",
    "Coding directly in **Tensorflow** allows you to tweak more parameters to optimize performance. The **Keras** wrapper makes the code more accessible for developers prototyping models.\n",
    "\n",
    "![levers](img/levers.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3bD0TmweRV8q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRrR9M9tRV8r"
   },
   "source": [
    "### TensorFlow vs Keras example with Image Classification\n",
    "[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) is a common benchmark in machine learning for image recognition. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. \n",
    "\n",
    "<img align =left src=\"img/c10imgcat.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZdgf77_RV8s"
   },
   "source": [
    "<img align =left src=\"img/tf-logo3.png\"><br>\n",
    "\n",
    "\n",
    "This task can be accomplished in TensorFlow and the code can be found [here](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10/). Let's look at it together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-lBsmlvRV8t"
   },
   "source": [
    "<img align =left src=\"img/keras.png\"><br>\n",
    "\n",
    "Now, the same task in [in Keras](https://keras.io/examples/cifar10_cnn/). After looking at it and comparing it to the tensorflow code, what do you notice? Any formatting look familiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IcfWb84RV8t"
   },
   "source": [
    "# Another Problem: Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rL_zg_IRV8u"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNodFAvFRV8w"
   },
   "source": [
    "For this lesson we'll use sklearn's built-in breast cancer dataset. The next cell loads the data and prints the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRfOgFn4RV8w"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "print(data.DESCR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Du2mEDHCRV8y"
   },
   "outputs": [],
   "source": [
    "# Splitting our data and initializing a Scaler\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNPgAR0tRV82"
   },
   "outputs": [],
   "source": [
    "# Transforming our data\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBpVF3bURV85"
   },
   "source": [
    "## Constructing a Neural Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlxJQtr5RV85"
   },
   "outputs": [],
   "source": [
    "# Importing model and layer types\n",
    "model = Sequential()\n",
    "model.add(Dense(30 , activation= 'relu', input_dim=30))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Importing our optimizer\n",
    "\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCbtBINzRV88"
   },
   "outputs": [],
   "source": [
    "# Constructing and compiling our model\n",
    "# - 3 layers, first uses relu, second sigmoid\n",
    "# - input layer and hidden layer have the same amount of nodes\n",
    "\n",
    "\n",
    "# use the correct loss function for binary classification\n",
    "adam = Adam()\n",
    "model.compile(optimizer=adam, ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Cwq3Ex_RV8-"
   },
   "outputs": [],
   "source": [
    "# Fitting our model\n",
    "\n",
    "\n",
    "# 50 epochs\n",
    "# batch size 200\n",
    "# use validation data when fitting your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9flW2PmRV9A"
   },
   "outputs": [],
   "source": [
    "# Storing that fit as a history log\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=50, verbose=0, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-1mnffY3RV9C"
   },
   "outputs": [],
   "source": [
    "# Plotting our losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPY6YcGpRV9D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEr4_YpPRV9F"
   },
   "source": [
    "## Appendix: Constructing a Neural Network in (\"Base\") Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArGKFFhyRV9F"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioLm05D5RV9H"
   },
   "outputs": [],
   "source": [
    "var_1 = tf.Variable(3)\n",
    "var_2 = tf.Variable(2)\n",
    "\n",
    "var_3 = var_1 * var_2\n",
    "var_4 = var_1 + var_3 * var_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ha9fqZveRV9I"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = sess.run(var_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7x0fguDRV9K"
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyOwiwM4RV9N"
   },
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dITpYNIARV9Q"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, 30))\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "hid = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "y_hat = tf.layers.dense(hid, 1, activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "training_run = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7FVyv5FRV9S"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train_scaled, y: y_train.reshape(-1, 1)})\n",
    "        \n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})\n",
    "\n",
    "classes = (pred > 0.5).astype(int)\n",
    "\n",
    "metrics.accuracy_score(y_test.reshape(-1, 1), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vh0VfUlGRV8T"
   },
   "outputs": [],
   "source": [
    "adam = Adam()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64 , activation= 'relu'  , input_dim=64))\n",
    "model.add(Dense(4,  activation= 'relu' ))\n",
    "model.add(Dense(1 , activation = 'sigmoid'  ))\n",
    "\n",
    "model.compile(optimizer= adam,\n",
    "              loss= 'binary_crossentropy' ,\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, verbose = 1, epochs= 50, batch_size= 10)\n",
    "# X_train, X_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "intro-to-keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
